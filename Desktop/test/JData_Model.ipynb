{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_feature_b_V1.csv')\n",
    "test = pd.read_csv('test_feature_b_V1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = lgb.Dataset(train.drop(['uid','label'],axis=1),\n",
    "                     label=train.label)\n",
    "dtest = lgb.Dataset(test.drop(['uid'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params =  {\n",
    "    'max_bin':10,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "#    'metric': ('multi_logloss', 'multi_error'),\n",
    "    #'metric_freq': 100,\n",
    "    'is_training_metric': False,\n",
    "    'min_data_in_leaf': 16,\n",
    "    'num_leaves': 256,\n",
    "    'learning_rate': 0.04,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    \n",
    "    'verbosity':-1,\n",
    "#    'gpu_device_id':2,\n",
    "#    'device':'gpu'\n",
    "    #'lambda_l1': 0.001,\n",
    "    'skip_drop': 0.95,\n",
    "    # max_drop 改小后，结果变好\n",
    "    'max_drop' : 8,\n",
    "    'lambda_l2': 0.005\n",
    "\n",
    "    #'num_threads': 18\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM 使用 leaf-wise 的树生长策略, 而很多其他流行的算法采用 depth-wise 的树生长策略.   \n",
    "与 depth-wise 的树生长策略相较, leaf-wise 算法可以收敛的更快.    \n",
    "但是, 如果参数选择不当的话, leaf-wise 算法有可能导致过拟合.  \n",
    "\n",
    "http://lightgbm.apachecn.org/cn/latest/Parameters-Tuning.html#leaf-wise\n",
    "\n",
    "\n",
    "num_leaves. 这是控制树模型复杂度的主要参数. 理论上, 借鉴 depth-wise 树, 我们可以设置 num_leaves = 2^(max_depth) 但是, 这种简单的转化在实际应用中表现不佳. 这是因为, 当叶子数目相同时, leaf-wise 树要比 depth-wise 树深得多, 这就有可能导致过拟合. 因此, 当我们试着调整 num_leaves 的取值时, 应该让其小于 2^(max_depth). 举个例子, 当 max_depth=6 时(这里译者认为例子中, 树的最大深度应为7), depth-wise 树可以达到较高的准确率.但是如果设置 num_leaves 为 127 时, 有可能会导致过拟合, 而将其设置为 70 或 80 时可能会得到比 depth-wise 树更高的准确率. 其实, depth 的概念在 leaf-wise 树中并没有多大作用, 因为并不存在一个从 leaves 到 depth 的合理映射.\n",
    "\n",
    "min_data_in_leaf. 这是处理 leaf-wise 树的过拟合问题中一个非常重要的参数. 它的值取决于训练数据的样本个树和 num_leaves. 将其设置的较大可以避免生成一个过深的树, 但有可能导致欠拟合. 实际应用中, 对于大数据集, 设置其为几百或几千就足够了.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本地 CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalMetric(preds,dtrain):\n",
    "    \n",
    "    label = dtrain.get_label()\n",
    "    \n",
    "    \n",
    "    pre = pd.DataFrame({'preds':preds,'label':label})\n",
    "    pre= pre.sort_values(by='preds',ascending=False)\n",
    "    \n",
    "    auc = metrics.roc_auc_score(pre.label,pre.preds)\n",
    "\n",
    "    pre.preds=pre.preds.map(lambda x: 1 if x>=0.5 else 0)\n",
    "\n",
    "    f1 = metrics.f1_score(pre.label,pre.preds)\n",
    "    \n",
    "    \n",
    "    res = 0.6*auc +0.4*f1\n",
    "    \n",
    "    return 'res',res,True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\tcv_agg's res: 0.741994 + 0.00997265\n",
      "[10]\tcv_agg's res: 0.74867 + 0.00935533\n",
      "[15]\tcv_agg's res: 0.752296 + 0.0088537\n",
      "[20]\tcv_agg's res: 0.750784 + 0.00697227\n",
      "[25]\tcv_agg's res: 0.757434 + 0.00733749\n",
      "[30]\tcv_agg's res: 0.758333 + 0.0098986\n",
      "[35]\tcv_agg's res: 0.757776 + 0.00602476\n",
      "[40]\tcv_agg's res: 0.758035 + 0.00555178\n",
      "[45]\tcv_agg's res: 0.760469 + 0.00693763\n",
      "[50]\tcv_agg's res: 0.763328 + 0.00573268\n",
      "[55]\tcv_agg's res: 0.767601 + 0.00840056\n",
      "[60]\tcv_agg's res: 0.768559 + 0.00926277\n",
      "[65]\tcv_agg's res: 0.769856 + 0.00996407\n",
      "[70]\tcv_agg's res: 0.770707 + 0.0111501\n",
      "[75]\tcv_agg's res: 0.771366 + 0.0110921\n",
      "[80]\tcv_agg's res: 0.772848 + 0.0104222\n",
      "[85]\tcv_agg's res: 0.774597 + 0.00931024\n",
      "[90]\tcv_agg's res: 0.774319 + 0.00873744\n",
      "[95]\tcv_agg's res: 0.773952 + 0.00859013\n",
      "[100]\tcv_agg's res: 0.773842 + 0.00969116\n",
      "[105]\tcv_agg's res: 0.774777 + 0.0112883\n",
      "[110]\tcv_agg's res: 0.772067 + 0.0103625\n",
      "[115]\tcv_agg's res: 0.772607 + 0.00959932\n",
      "[120]\tcv_agg's res: 0.775898 + 0.0107378\n",
      "[125]\tcv_agg's res: 0.776638 + 0.0109509\n",
      "[130]\tcv_agg's res: 0.775503 + 0.00936077\n",
      "[135]\tcv_agg's res: 0.776826 + 0.00981192\n",
      "[140]\tcv_agg's res: 0.776179 + 0.0101933\n",
      "[145]\tcv_agg's res: 0.776222 + 0.00992599\n",
      "[150]\tcv_agg's res: 0.777808 + 0.00954636\n",
      "[155]\tcv_agg's res: 0.776099 + 0.0089231\n",
      "[160]\tcv_agg's res: 0.775833 + 0.0118451\n",
      "[165]\tcv_agg's res: 0.776945 + 0.0107956\n",
      "[170]\tcv_agg's res: 0.775872 + 0.010889\n",
      "[175]\tcv_agg's res: 0.776818 + 0.0123288\n",
      "[180]\tcv_agg's res: 0.778682 + 0.0115663\n",
      "[185]\tcv_agg's res: 0.778805 + 0.0106192\n",
      "[190]\tcv_agg's res: 0.78003 + 0.0111807\n",
      "[195]\tcv_agg's res: 0.778448 + 0.00996359\n",
      "[200]\tcv_agg's res: 0.779233 + 0.00989818\n",
      "[205]\tcv_agg's res: 0.778553 + 0.00937273\n",
      "[210]\tcv_agg's res: 0.778185 + 0.0100264\n",
      "[215]\tcv_agg's res: 0.777424 + 0.00979988\n",
      "[220]\tcv_agg's res: 0.777061 + 0.00979248\n",
      "[225]\tcv_agg's res: 0.776986 + 0.00779247\n",
      "[230]\tcv_agg's res: 0.778159 + 0.00887564\n",
      "[235]\tcv_agg's res: 0.778718 + 0.00981694\n",
      "[240]\tcv_agg's res: 0.777933 + 0.00894732\n",
      "[245]\tcv_agg's res: 0.777519 + 0.0102323\n",
      "[250]\tcv_agg's res: 0.778441 + 0.00935374\n",
      "[255]\tcv_agg's res: 0.777825 + 0.0107861\n",
      "[260]\tcv_agg's res: 0.779129 + 0.0109843\n",
      "[265]\tcv_agg's res: 0.777517 + 0.0117332\n",
      "[270]\tcv_agg's res: 0.777674 + 0.0116\n",
      "[275]\tcv_agg's res: 0.776837 + 0.0120107\n",
      "[280]\tcv_agg's res: 0.776072 + 0.0119292\n",
      "[285]\tcv_agg's res: 0.775984 + 0.0114682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'res-mean': [0.7347819956501654,\n",
       "  0.7408059057564426,\n",
       "  0.7376535919002555,\n",
       "  0.7394114305579377,\n",
       "  0.7419936580784926,\n",
       "  0.7456802642453226,\n",
       "  0.7489591076341009,\n",
       "  0.7497662661528685,\n",
       "  0.7500842654260126,\n",
       "  0.7486703340004114,\n",
       "  0.7487834771786627,\n",
       "  0.7512824652281008,\n",
       "  0.7505891671627946,\n",
       "  0.7518017910724932,\n",
       "  0.7522956707446582,\n",
       "  0.7496138493500505,\n",
       "  0.7502712481130546,\n",
       "  0.7524813555300286,\n",
       "  0.7543234725979312,\n",
       "  0.7507838445683813,\n",
       "  0.7542791856136747,\n",
       "  0.7556441976019806,\n",
       "  0.7570891159753167,\n",
       "  0.7564432365886273,\n",
       "  0.7574343760623347,\n",
       "  0.7582751161570149,\n",
       "  0.7566461599112936,\n",
       "  0.7592739045646212,\n",
       "  0.7599929376453445,\n",
       "  0.7583327346507254,\n",
       "  0.7597162764668107,\n",
       "  0.7608125983806039,\n",
       "  0.7593148578059296,\n",
       "  0.7616029000454446,\n",
       "  0.7577760424699514,\n",
       "  0.7595541476187652,\n",
       "  0.7598927206085492,\n",
       "  0.7596064302557308,\n",
       "  0.7601347118990126,\n",
       "  0.7580350902465126,\n",
       "  0.7609240447027101,\n",
       "  0.7607401257738382,\n",
       "  0.7619807893296121,\n",
       "  0.760848660213253,\n",
       "  0.7604689393042356,\n",
       "  0.7637483402172567,\n",
       "  0.7642710786345249,\n",
       "  0.7626605420829585,\n",
       "  0.7636034282314133,\n",
       "  0.7633284596567927,\n",
       "  0.7630315605153672,\n",
       "  0.7658705739126958,\n",
       "  0.7655217090942622,\n",
       "  0.7670340874541429,\n",
       "  0.7676012121237407,\n",
       "  0.7682453578295685,\n",
       "  0.7684364416842043,\n",
       "  0.7689343944916999,\n",
       "  0.7681912343841409,\n",
       "  0.7685589738522732,\n",
       "  0.769546283201243,\n",
       "  0.7711711834821253,\n",
       "  0.7698855556341782,\n",
       "  0.7704475758739369,\n",
       "  0.7698555820513876,\n",
       "  0.7696785875420479,\n",
       "  0.7705893170206455,\n",
       "  0.770853821602086,\n",
       "  0.7717503447600752,\n",
       "  0.7707069184087013,\n",
       "  0.7715539328963045,\n",
       "  0.7706737693625104,\n",
       "  0.7695588993901898,\n",
       "  0.7698923317617439,\n",
       "  0.7713663968251235,\n",
       "  0.7719032641930426,\n",
       "  0.7715294423477469,\n",
       "  0.7714543602067963,\n",
       "  0.7722035526848948,\n",
       "  0.7728476436228432,\n",
       "  0.7721426540948477,\n",
       "  0.7725260215340404,\n",
       "  0.7723965628844374,\n",
       "  0.7747826296612349,\n",
       "  0.7745974191856898,\n",
       "  0.774246015732623,\n",
       "  0.7740490117719228,\n",
       "  0.7755153429050301,\n",
       "  0.7754024378765173,\n",
       "  0.7743189720301329,\n",
       "  0.7744050057668647,\n",
       "  0.7750319675154795,\n",
       "  0.7739988802242209,\n",
       "  0.774543761170761,\n",
       "  0.7739522721801017,\n",
       "  0.7734575787757693,\n",
       "  0.7739039949788403,\n",
       "  0.7739225818343606,\n",
       "  0.773813617738805,\n",
       "  0.7738424567174554,\n",
       "  0.7742616803067128,\n",
       "  0.7736629885616301,\n",
       "  0.7740794224030605,\n",
       "  0.7741804797531341,\n",
       "  0.7747774576134048,\n",
       "  0.773359781592493,\n",
       "  0.7730088941240032,\n",
       "  0.773237895168663,\n",
       "  0.7728653508055845,\n",
       "  0.7720674305252606,\n",
       "  0.7727449237726324,\n",
       "  0.7729610809418269,\n",
       "  0.7729477888952075,\n",
       "  0.7726857943128808,\n",
       "  0.7726071437463191,\n",
       "  0.77316396984014,\n",
       "  0.7725917705822519,\n",
       "  0.7752267177518828,\n",
       "  0.7759348598748215,\n",
       "  0.7758977185049839,\n",
       "  0.7746445898297557,\n",
       "  0.7750191606154347,\n",
       "  0.774630846129189,\n",
       "  0.7756845479977127,\n",
       "  0.7766382357667933,\n",
       "  0.7759523480993047,\n",
       "  0.7754282516293397,\n",
       "  0.7756971530364395,\n",
       "  0.7757267130959953,\n",
       "  0.7755032963710736,\n",
       "  0.775471894962407,\n",
       "  0.7754106799668641,\n",
       "  0.7757295839451243,\n",
       "  0.7777202028509148,\n",
       "  0.7768262760528949,\n",
       "  0.7757046045998165,\n",
       "  0.7767012869693227,\n",
       "  0.7758077205921402,\n",
       "  0.7770984391936172,\n",
       "  0.7761786202486033,\n",
       "  0.7760792493044687,\n",
       "  0.7757007227507554,\n",
       "  0.7753788955478217,\n",
       "  0.7757575672452223,\n",
       "  0.7762215777277436,\n",
       "  0.7764550407384476,\n",
       "  0.7771560263439531,\n",
       "  0.7769210006485853,\n",
       "  0.777524881697282,\n",
       "  0.7778084433632083,\n",
       "  0.7767410093746886,\n",
       "  0.7763436269025465,\n",
       "  0.776052936403965,\n",
       "  0.7756755787433277,\n",
       "  0.7760986717774677,\n",
       "  0.7763020453909979,\n",
       "  0.7764804868750876,\n",
       "  0.7763148843685334,\n",
       "  0.7759905837138397,\n",
       "  0.7758331888014923,\n",
       "  0.7761069327020733,\n",
       "  0.776803578331119,\n",
       "  0.7772746726929469,\n",
       "  0.7768402523240475,\n",
       "  0.7769452542971275,\n",
       "  0.777228214189526,\n",
       "  0.7765157068764533,\n",
       "  0.7770835298331384,\n",
       "  0.7762352838373462,\n",
       "  0.7758717558260871,\n",
       "  0.7770230366989219,\n",
       "  0.7769247278355319,\n",
       "  0.7780913774641864,\n",
       "  0.777178025701597,\n",
       "  0.7768182855507885,\n",
       "  0.7773332266638574,\n",
       "  0.777365272876716,\n",
       "  0.7774929493779174,\n",
       "  0.7768742882709273,\n",
       "  0.7786816881006028,\n",
       "  0.7781609577691483,\n",
       "  0.7781824809940957,\n",
       "  0.7795678086531916,\n",
       "  0.7791348833006627,\n",
       "  0.7788050689644382,\n",
       "  0.7784447049466946,\n",
       "  0.7783092043690161,\n",
       "  0.7785475546219107,\n",
       "  0.7801269969582346],\n",
       " 'res-stdv': [0.008175088244942754,\n",
       "  0.007515081369383092,\n",
       "  0.01318111576374242,\n",
       "  0.014991301228022388,\n",
       "  0.009972648759124216,\n",
       "  0.00849230078743895,\n",
       "  0.005855503223339956,\n",
       "  0.006849347266218805,\n",
       "  0.008374013465538078,\n",
       "  0.009355326074414388,\n",
       "  0.008459289589483696,\n",
       "  0.009635145707031797,\n",
       "  0.008125634291670134,\n",
       "  0.008415598608916396,\n",
       "  0.008853697578749719,\n",
       "  0.008250868858487111,\n",
       "  0.007310204762235297,\n",
       "  0.008239723599634723,\n",
       "  0.0079044365433789,\n",
       "  0.0069722662537604705,\n",
       "  0.010029581086839651,\n",
       "  0.0078538250362459,\n",
       "  0.008438483458415576,\n",
       "  0.00866480593758,\n",
       "  0.00733749257325407,\n",
       "  0.00854801801603876,\n",
       "  0.00894952226517952,\n",
       "  0.007888055957855697,\n",
       "  0.008016333581997315,\n",
       "  0.009898595054063139,\n",
       "  0.00829680417895678,\n",
       "  0.008621025520762255,\n",
       "  0.00801040128202076,\n",
       "  0.006883680230023853,\n",
       "  0.006024762291366466,\n",
       "  0.006260646346519856,\n",
       "  0.007269541594126459,\n",
       "  0.0037937886297126876,\n",
       "  0.006016190034959346,\n",
       "  0.0055517806803945646,\n",
       "  0.0038813722574926254,\n",
       "  0.004723345219065261,\n",
       "  0.005609008556824113,\n",
       "  0.006791049962790043,\n",
       "  0.006937630199576743,\n",
       "  0.009244982856314123,\n",
       "  0.009236498251330643,\n",
       "  0.007009848785976767,\n",
       "  0.006932795682594845,\n",
       "  0.005732679916254713,\n",
       "  0.006123332116713975,\n",
       "  0.005954384746310258,\n",
       "  0.007901795292470854,\n",
       "  0.0075463619940125265,\n",
       "  0.008400561568456678,\n",
       "  0.00863660931197251,\n",
       "  0.010796515460361658,\n",
       "  0.009586730221331668,\n",
       "  0.008744613635609725,\n",
       "  0.009262769694664573,\n",
       "  0.009791364743574506,\n",
       "  0.009786452902855923,\n",
       "  0.010217970495352363,\n",
       "  0.008988723029662964,\n",
       "  0.009964071634832835,\n",
       "  0.009870451599436645,\n",
       "  0.010062189707882535,\n",
       "  0.009030150667869295,\n",
       "  0.010002669795944092,\n",
       "  0.01115014592442145,\n",
       "  0.011306275656013427,\n",
       "  0.010004155602965411,\n",
       "  0.009618298567706723,\n",
       "  0.010691618688822357,\n",
       "  0.011092104518561768,\n",
       "  0.010381887044983412,\n",
       "  0.01099562305295701,\n",
       "  0.011197133130116645,\n",
       "  0.010204040227976309,\n",
       "  0.010422198817589164,\n",
       "  0.010876225169746692,\n",
       "  0.009431302845440512,\n",
       "  0.00710502080705861,\n",
       "  0.008267827313108064,\n",
       "  0.009310239829494863,\n",
       "  0.009400731054505836,\n",
       "  0.010294859599895838,\n",
       "  0.00897272243089716,\n",
       "  0.008855267107018585,\n",
       "  0.008737436437630043,\n",
       "  0.009595413271436556,\n",
       "  0.008547406798340842,\n",
       "  0.009338298351660798,\n",
       "  0.009323250889618467,\n",
       "  0.008590126755996606,\n",
       "  0.008291856718606955,\n",
       "  0.00766400517545493,\n",
       "  0.008365344017949142,\n",
       "  0.009358795699560427,\n",
       "  0.00969115796042215,\n",
       "  0.009821098378150588,\n",
       "  0.009703453615587408,\n",
       "  0.010112616162999506,\n",
       "  0.010349730043833664,\n",
       "  0.011288299558652843,\n",
       "  0.011314921321813304,\n",
       "  0.01083745700465777,\n",
       "  0.011038341275008244,\n",
       "  0.010353296129475947,\n",
       "  0.010362492793326246,\n",
       "  0.009831284479352949,\n",
       "  0.011278984015913537,\n",
       "  0.009975198088187935,\n",
       "  0.010007837347843778,\n",
       "  0.009599319109276815,\n",
       "  0.009924497710586223,\n",
       "  0.010782709056230064,\n",
       "  0.01125334546952971,\n",
       "  0.011152550280550103,\n",
       "  0.0107378433644414,\n",
       "  0.00967726779511001,\n",
       "  0.00991660609811732,\n",
       "  0.0107169053753696,\n",
       "  0.011842843006011317,\n",
       "  0.010950902690509459,\n",
       "  0.010495780681553829,\n",
       "  0.01079505071471708,\n",
       "  0.011032769828273798,\n",
       "  0.009654932848743107,\n",
       "  0.009360765978503865,\n",
       "  0.009761635887605218,\n",
       "  0.009483124157425468,\n",
       "  0.010230086426121362,\n",
       "  0.010349061316314882,\n",
       "  0.00981192299224537,\n",
       "  0.010541783800325435,\n",
       "  0.009356980195671227,\n",
       "  0.01031125377474021,\n",
       "  0.009923698076104347,\n",
       "  0.010193289273576229,\n",
       "  0.010662337390680704,\n",
       "  0.009854994839376298,\n",
       "  0.01072525153196772,\n",
       "  0.010386474179004214,\n",
       "  0.009925988305737458,\n",
       "  0.009741357004718237,\n",
       "  0.009901710180249263,\n",
       "  0.010534962966705827,\n",
       "  0.00971043753078308,\n",
       "  0.009546364589733026,\n",
       "  0.010382760446198167,\n",
       "  0.01182345948074857,\n",
       "  0.01004335696411072,\n",
       "  0.010160446359034896,\n",
       "  0.008923103217331349,\n",
       "  0.012030053989561723,\n",
       "  0.011278306795883486,\n",
       "  0.010693151704806541,\n",
       "  0.008870402582760666,\n",
       "  0.011845133332207129,\n",
       "  0.011881609935116392,\n",
       "  0.010566590004391401,\n",
       "  0.011548251765735524,\n",
       "  0.011037190876968018,\n",
       "  0.010795575731776167,\n",
       "  0.009763466375316871,\n",
       "  0.011538464341850248,\n",
       "  0.011215920405356829,\n",
       "  0.012203273678872053,\n",
       "  0.01088904766888573,\n",
       "  0.011496057877316505,\n",
       "  0.011884052375169873,\n",
       "  0.012530715834770823,\n",
       "  0.01270210566731637,\n",
       "  0.01232882944556341,\n",
       "  0.01323946025110286,\n",
       "  0.011687663684370872,\n",
       "  0.012022331218539207,\n",
       "  0.012467352754131814,\n",
       "  0.011566297627460727,\n",
       "  0.012053006348647173,\n",
       "  0.011402954820199533,\n",
       "  0.012006323862007628,\n",
       "  0.010943531345965535,\n",
       "  0.010619167350816439,\n",
       "  0.009326050818136849,\n",
       "  0.009937564399738358,\n",
       "  0.01126293779071437,\n",
       "  0.011609919575939874]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb.cv(lgb_params,\n",
    "       dtrain,\n",
    "       feval=evalMetric,\n",
    "       early_stopping_rounds=100,\n",
    "       verbose_eval=5,\n",
    "       num_boost_round=500,\n",
    "       nfold=3,\n",
    "       metrics=['evalMetric'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\ttraining's res: 0.875228\n",
      "[10]\ttraining's res: 0.89033\n",
      "[15]\ttraining's res: 0.902898\n",
      "[20]\ttraining's res: 0.911706\n",
      "[25]\ttraining's res: 0.919474\n",
      "[30]\ttraining's res: 0.926706\n",
      "[35]\ttraining's res: 0.935386\n",
      "[40]\ttraining's res: 0.940653\n",
      "[45]\ttraining's res: 0.947782\n",
      "[50]\ttraining's res: 0.953037\n",
      "[55]\ttraining's res: 0.960141\n",
      "[60]\ttraining's res: 0.966362\n",
      "[65]\ttraining's res: 0.972117\n",
      "[70]\ttraining's res: 0.976593\n",
      "[75]\ttraining's res: 0.979175\n",
      "[80]\ttraining's res: 0.984851\n",
      "[85]\ttraining's res: 0.987587\n",
      "[90]\ttraining's res: 0.990726\n",
      "[95]\ttraining's res: 0.991969\n",
      "[100]\ttraining's res: 0.9952\n",
      "[105]\ttraining's res: 0.996369\n",
      "[110]\ttraining's res: 0.997288\n",
      "[115]\ttraining's res: 0.9982\n",
      "[120]\ttraining's res: 0.998655\n",
      "[125]\ttraining's res: 0.99866\n",
      "[130]\ttraining's res: 0.999108\n",
      "[135]\ttraining's res: 0.999332\n",
      "[140]\ttraining's res: 0.999333\n",
      "[145]\ttraining's res: 0.999333\n",
      "[150]\ttraining's res: 0.999555\n",
      "[155]\ttraining's res: 0.999555\n",
      "[160]\ttraining's res: 1\n",
      "[165]\ttraining's res: 1\n",
      "[170]\ttraining's res: 1\n",
      "[175]\ttraining's res: 1\n",
      "[180]\ttraining's res: 1\n",
      "[185]\ttraining's res: 1\n",
      "[190]\ttraining's res: 1\n",
      "[195]\ttraining's res: 1\n",
      "[200]\ttraining's res: 1\n",
      "[205]\ttraining's res: 1\n",
      "[210]\ttraining's res: 1\n",
      "[215]\ttraining's res: 1\n",
      "[220]\ttraining's res: 1\n",
      "[225]\ttraining's res: 1\n",
      "[230]\ttraining's res: 1\n",
      "[235]\ttraining's res: 1\n",
      "[240]\ttraining's res: 1\n",
      "[245]\ttraining's res: 1\n",
      "[250]\ttraining's res: 1\n",
      "[255]\ttraining's res: 1\n",
      "[260]\ttraining's res: 1\n",
      "[265]\ttraining's res: 1\n",
      "[270]\ttraining's res: 1\n",
      "[275]\ttraining's res: 1\n",
      "[280]\ttraining's res: 1\n",
      "[285]\ttraining's res: 1\n",
      "[290]\ttraining's res: 1\n",
      "[295]\ttraining's res: 1\n",
      "[300]\ttraining's res: 1\n",
      "[305]\ttraining's res: 1\n",
      "[310]\ttraining's res: 1\n",
      "[315]\ttraining's res: 1\n",
      "[320]\ttraining's res: 1\n",
      "[325]\ttraining's res: 1\n",
      "[330]\ttraining's res: 1\n",
      "[335]\ttraining's res: 1\n",
      "[340]\ttraining's res: 1\n",
      "[345]\ttraining's res: 1\n",
      "[350]\ttraining's res: 1\n",
      "[355]\ttraining's res: 1\n",
      "[360]\ttraining's res: 1\n",
      "[365]\ttraining's res: 1\n",
      "[370]\ttraining's res: 1\n",
      "[375]\ttraining's res: 1\n",
      "[380]\ttraining's res: 1\n",
      "[385]\ttraining's res: 1\n",
      "[390]\ttraining's res: 1\n",
      "[395]\ttraining's res: 1\n",
      "[400]\ttraining's res: 1\n",
      "[405]\ttraining's res: 1\n",
      "[410]\ttraining's res: 1\n",
      "[415]\ttraining's res: 1\n",
      "[420]\ttraining's res: 1\n",
      "[425]\ttraining's res: 1\n",
      "[430]\ttraining's res: 1\n",
      "[435]\ttraining's res: 1\n",
      "[440]\ttraining's res: 1\n",
      "[445]\ttraining's res: 1\n",
      "[450]\ttraining's res: 1\n",
      "[455]\ttraining's res: 1\n",
      "[460]\ttraining's res: 1\n",
      "[465]\ttraining's res: 1\n",
      "[470]\ttraining's res: 1\n",
      "[475]\ttraining's res: 1\n",
      "[480]\ttraining's res: 1\n",
      "[485]\ttraining's res: 1\n",
      "[490]\ttraining's res: 1\n",
      "[495]\ttraining's res: 1\n",
      "[500]\ttraining's res: 1\n"
     ]
    }
   ],
   "source": [
    "model =lgb.train(lgb_params,\n",
    "                 dtrain,\n",
    "                 feval=evalMetric,\n",
    "                 verbose_eval=5,\n",
    "                 num_boost_round = 500,\n",
    "                 valid_sets=[dtrain])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(test.drop(['uid'],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res =pd.DataFrame({'uid':test.uid,'label':pred})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=res.sort_values(by='label',ascending=False)\n",
    "res.label=res.label.map(lambda x: 1 if x>=0.5 else 0)\n",
    "res.label = res.label.map(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('b-v2-last.csv',index=False,header=False,\n",
    "           sep=',',columns=['uid','label'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
